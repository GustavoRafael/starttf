{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Starttf.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "5U1bPetVYK4G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Start Tensorflow with starttf\n",
        "\n",
        "First we need to install tensorflow and check if it works."
      ]
    },
    {
      "metadata": {
        "id": "YKRSAMwmWWzn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7gE9-ECjngu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61416133-dfc5-4316-8c49-e11a8e32036a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525532422569,
          "user_tz": -120,
          "elapsed": 525,
          "user": {
            "displayName": "Michael PenguinMenace",
            "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
            "userId": "112772895502545919169"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KLjN3csHXFf2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next let's install starttf and opendatalake"
      ]
    },
    {
      "metadata": {
        "id": "Y8HotiWuYv1p",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/penguinmenac3/starttf/archive/master.zip\n",
        "!pip install https://github.com/penguinmenac3/opendatalake/archive/master.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "as-_GHVsY-LQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Starting out\n",
        "\n",
        "## Loading a dataset\n",
        "\n",
        "Let's start by loading a dataset.\n",
        "The simplest dataset for beginners is mnist.\n",
        "The good thing about mnist is, that you do not need any complex downloading code.\n",
        "Simply load it via the opendatalake."
      ]
    },
    {
      "metadata": {
        "id": "LNlTt3s5b8g0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from opendatalake.classification.mnist import mnist\n",
        "\n",
        "base_dir = \"data/mnist\"\n",
        "\n",
        "# Get a generator and its parameters\n",
        "train_gen, train_gen_params = mnist(base_dir=base_dir, phase=\"train\")\n",
        "validation_gen, validation_gen_params = mnist(base_dir=base_dir, phase=\"validation\")\n",
        "\n",
        "# Create a generator to see some images\n",
        "data = train_gen(train_gen_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K-OR1AOQdDbF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you have downloaded the dataset and a generator which will output you labels and features.\n",
        "Let's inspect some features and labels!"
      ]
    },
    {
      "metadata": {
        "id": "yjgnNJZWdML9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb0affbe-9a27-4b6e-de04-54a7abb51f4a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525532442922,
          "user_tz": -120,
          "elapsed": 749,
          "user": {
            "displayName": "Michael PenguinMenace",
            "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
            "userId": "112772895502545919169"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "features, labels = next(data)\n",
        "print(features.keys())\n",
        "print(labels.keys())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['image'])\n",
            "dict_keys(['probs'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PbyB-19rdazt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The image is a numpy array. Let's plot it using matplotlib. The label is one hot encoded probabilities. Using np.argmax you can receive the label of the image.\n",
        "\n",
        "In the case of mnist the image is a 1d-array with 786 values. However, it actually represents a (28,28) image. So first you have to reshape it using numpy.\n",
        "\n",
        "The Label is one hot encoded, this means you can use `np.argmax` to retrieve the index at which the one is (aka the label in human readable form).\n",
        "\n",
        "Finally plot the image using matplotlibs imshow."
      ]
    },
    {
      "metadata": {
        "id": "eVREumamdY8d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "de3fd70c-ba94-4904-9104-f3a7f533cff2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525532443744,
          "user_tz": -120,
          "elapsed": 647,
          "user": {
            "displayName": "Michael PenguinMenace",
            "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
            "userId": "112772895502545919169"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reshape image from (786,) to (28,28)\n",
        "img = np.reshape(features['image'], (28,28))\n",
        "number = np.argmax(labels['probs'])\n",
        "\n",
        "# Plot img with number as title\n",
        "plt.title(\"Number: %d\" % number)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd9a881d550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_FkCrnqEgz3x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters object\n",
        "\n",
        "We need a hyperparams object to store all hyperparamters we setup for our training. Let's create one where we can add all variables."
      ]
    },
    {
      "metadata": {
        "id": "aTkJkkc3g90g",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from starttf.utils.dict2obj import Dict2Obj\n",
        "\n",
        "hyper_params = Dict2Obj({})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ImAClLTFfFc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing a training\n",
        "\n",
        "Before writing a model, loss and everything fancy, you need to prepare your data for training.\n",
        "\n",
        "In the case of mnist no cleaning or augmentation is required, so we can simply write the data into a tfrecord file.\n",
        "\n",
        "However, to illustrate how data augmentation could work, we will set the data augmentation steps to 1 manually.\n",
        "This involves adding a problem parameter to our hyper parameters."
      ]
    },
    {
      "metadata": {
        "id": "y5CfwwFgf_he",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from starttf.tfrecords.autorecords import write_data\n",
        "\n",
        "# Let's define our learning problem\n",
        "hyper_params.problem = Dict2Obj({\"augmentation\": {\"steps\": 1}})\n",
        "\n",
        "write_data(hyper_params, \"records/train\", train_gen, train_gen_params, 4)\n",
        "write_data(hyper_params, \"records/validation\", validation_gen, validation_gen_params, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zbYrSStLwzpU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a model (the easy way)\n",
        "\n",
        "Now that the data is written into a format that we can efficiently read for training, let's have a look at an easy way to train a model.\n",
        "\n",
        "First let's create a dict where to gather all hyperparameters for training. Usually you would put that in an extra .json file which can be loaded easily."
      ]
    },
    {
      "metadata": {
        "id": "8cw2zI7uyXet",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "hyper_params_dict = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JOmUzd22ybaZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For simplicity we will use a predefined model (we will later see how to write a create model function by ourselves).\n",
        "\n",
        "This model has a hyperparameter for `dropout_rate`"
      ]
    },
    {
      "metadata": {
        "id": "lXeNw6YAxNDw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from starttf.models.mnist import create_model as mnist_model\n",
        "\n",
        "hyper_params_dict[\"arch\"] = {\"dropout_rate\": 0.5}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wbIGqcsxXLd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we need to define a loss.\n",
        "\n",
        "The loss glues together our labels and the model.\n",
        "\n",
        "In the case of mnist it is a simple cross entropy loss."
      ]
    },
    {
      "metadata": {
        "id": "h1hHVuwoxVkT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from starttf.utils.misc import mode_to_str\n",
        "\n",
        "def create_loss(model, labels, mode, hyper_params):\n",
        "    mode_name = mode_to_str(mode)\n",
        "    metrics = {}\n",
        "\n",
        "    # Add loss\n",
        "    labels = tf.reshape(labels[\"probs\"], [-1, 10])\n",
        "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model[\"logits\"], labels=labels))\n",
        "    # For tensorboard, we don't use that now.\n",
        "    #tf.summary.scalar(mode_name + '/loss', loss_op)\n",
        "    metrics[mode_name + '/loss'] = loss_op\n",
        "\n",
        "    return loss_op, metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UbGeBGXxxVYd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we need to define all our hyperparameters and launch the training."
      ]
    },
    {
      "metadata": {
        "id": "dqggqmHNxv3D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "51ce8d74-a215-4414-fa1a-4710a4e8999a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525532672142,
          "user_tz": -120,
          "elapsed": 192886,
          "user": {
            "displayName": "Michael PenguinMenace",
            "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
            "userId": "112772895502545919169"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "hyper_params_dict[\"train\"] = {}\n",
        "hyper_params_dict[\"train\"][\"checkpoint_path\"] = \"checkpoints\"\n",
        "hyper_params_dict[\"train\"][\"iters\"] = 2000\n",
        "hyper_params_dict[\"train\"][\"summary_iters\"] = 250\n",
        "hyper_params_dict[\"train\"][\"tf_records_path\"] = \"records\"\n",
        "hyper_params_dict[\"train\"][\"batch_size\"] = 64\n",
        "hyper_params_dict[\"train\"][\"validation_batch_size\"] = 64\n",
        "hyper_params_dict[\"train\"][\"learning_rate\"] = {\"type\": \"const\", \"start_value\": 0.001}\n",
        "hyper_params_dict[\"train\"][\"optimizer\"] = {\"type\": \"adam\"}\n",
        "\n",
        "\n",
        "hyper_params = Dict2Obj(hyper_params_dict)\n",
        "\n",
        "from starttf.estimators.scientific_estimator import easy_train_and_evaluate\n",
        "\n",
        "easy_train_and_evaluate(hyper_params, mnist_model, create_loss)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Create training graph\n",
            "WARNING:tensorflow:From <ipython-input-11-8093b2b7578f>:9: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "Create validation graph\n",
            "Start training\n",
            "Training Model: To reduce overhead no outputs are done. Use tensorboard to see your progress.\n",
            "python -m tensorboard.main --logdir=checkpoints\n",
            "Timing: 162.592 ms per iteration\n",
            "Iter: 250, Train loss: 0.0691, Test loss: 0.1521\n",
            "Timing: 95.804 ms per iteration\n",
            "Iter: 500, Train loss: 0.0873, Test loss: 0.0662\n",
            "Timing: 95.678 ms per iteration\n",
            "Iter: 750, Train loss: 0.0938, Test loss: 0.0943\n",
            "Timing: 95.158 ms per iteration\n",
            "Iter: 1000, Train loss: 0.0268, Test loss: 0.1433\n",
            "Timing: 96.877 ms per iteration\n",
            "Iter: 1250, Train loss: 0.0831, Test loss: 0.0396\n",
            "Timing: 96.615 ms per iteration\n",
            "Iter: 1500, Train loss: 0.0307, Test loss: 0.1409\n",
            "Timing: 95.890 ms per iteration\n",
            "Iter: 1750, Train loss: 0.0171, Test loss: 0.1426\n",
            "Timing: 95.224 ms per iteration\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd9ab37a6a0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "l5vZRDeY5rb8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you run this code on your native machine, you can visit the checkpoints path and find images there which contain plots of your metrics. In this case the loss."
      ]
    },
    {
      "metadata": {
        "id": "pjVt8y9mr87M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training a model (a more involved way)\n",
        "\n",
        "After having trained a model the easy way, let's do it the more involved way. In some cases the easy_train_and_evaluate might not fit your needs.\n",
        "\n",
        "For training basically 4 steps are required:\n",
        "\n",
        "1. Loading the data into a tensor\n",
        "2. Defining the model\n",
        "3. Defining the loss\n",
        "4. Train and Evaluate the model given the loss\n",
        "\n",
        "Then let's start with loading the data into a tensor."
      ]
    },
    {
      "metadata": {
        "id": "Mn2UIIVwizC6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from starttf.tfrecords.autorecords import read_data\n",
        "\n",
        "train_features, train_labels = read_data(\"records/train\", 64)\n",
        "validation_features, validation_labels = read_data(\"records/validation\", 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y-D3lhx_tfah",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we define a model using those tensors.\n",
        "Creating a model works by passing in an input_tensor, mode, and hyper params.\n",
        "\n",
        "The mode tells the network if it has to run in evaluation, prediction or training mode.\n",
        "When in eval mode, we want to resue the weights from the training network.\n",
        "Otherwise the network would train and evaluate using different weights.\n",
        "\n",
        "The Network we want to write is a little bit ispired by vgg just smaller. We have 2 conv layers, a pooling layer, 2 conv layers a pooling layer, dropout in training and finally some fully connected layer before a softmax layer."
      ]
    },
    {
      "metadata": {
        "id": "accAmakd0_sf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def create_model(input_tensor, mode, hyper_params):\n",
        "    model = {}\n",
        "    \n",
        "    with tf.variable_scope('SimpleMnistNetwork') as scope:\n",
        "        if mode == tf.estimator.ModeKeys.EVAL:\n",
        "            scope.reuse_variables()\n",
        "\n",
        "        # Prepare the inputs\n",
        "        x = tf.reshape(tensor=input_tensor[\"image\"], shape=(-1, 28, 28, 1), name=\"input\")\n",
        "\n",
        "        # First Conv Block\n",
        "        conv1 = tf.layers.conv2d(inputs=x, filters=16, kernel_size=(3, 3), strides=(1, 1), name=\"conv1\",\n",
        "                                 activation=tf.nn.relu)\n",
        "        conv2 = tf.layers.conv2d(inputs=conv1, filters=32, kernel_size=(3, 3), strides=(1, 1), name=\"conv2\",\n",
        "                                 activation=tf.nn.relu)\n",
        "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=(2, 2), strides=(2, 2), name=\"pool2\")\n",
        "\n",
        "        # Second Conv Block\n",
        "        conv3 = tf.layers.conv2d(inputs=pool2, filters=32, kernel_size=(3, 3), strides=(1, 1), name=\"conv3\",\n",
        "                                 activation=tf.nn.relu)\n",
        "        conv4 = tf.layers.conv2d(inputs=conv3, filters=32, kernel_size=(3, 3), strides=(1, 1), name=\"conv4\",\n",
        "                                 activation=tf.nn.relu)\n",
        "        pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=(2, 2), strides=(2, 2), name=\"pool4\")\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            pool4 = tf.layers.dropout(inputs=pool4, rate=hyper_params.arch.dropout_rate, name=\"drop4\")\n",
        "\n",
        "        # Fully Connected Block\n",
        "        probs = tf.layers.flatten(inputs=pool4)\n",
        "        logits = tf.layers.dense(inputs=probs, units=10, activation=None, name=\"logits\")\n",
        "        probs = tf.nn.softmax(logits=logits, name=\"probs\")\n",
        "\n",
        "        # Collect outputs for api of network.\n",
        "        model[\"pool2\"] = pool2\n",
        "        model[\"pool4\"] = pool4\n",
        "        model[\"logits\"] = logits\n",
        "        model[\"probs\"] = probs\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pmBR5Vxs1AH9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have to create our model twice, once for training and once for evaluation."
      ]
    },
    {
      "metadata": {
        "id": "Hvy79ETqtpTm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_model = create_model(train_features, tf.estimator.ModeKeys.TRAIN, hyper_params)\n",
        "validation_model = create_model(validation_features, tf.estimator.ModeKeys.EVAL, hyper_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gPKr0BnuPzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As a loss we can use the same as for the easy training way.\n",
        "We need to create it once for training and once for validation."
      ]
    },
    {
      "metadata": {
        "id": "soQXdz6suwlH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_loss, train_metrics = create_loss(train_model, train_labels, tf.estimator.ModeKeys.TRAIN, hyper_params)\n",
        "validation_loss, validation_metrics = create_loss(validation_model, validation_labels, tf.estimator.ModeKeys.EVAL, hyper_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "maYcpu93vJ2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we need to define a training operation to be able to train our model. In this case we will use a default adam optimizer, it should work ok."
      ]
    },
    {
      "metadata": {
        "id": "QqBXoXHdvQ7F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_op = tf.train.AdamOptimizer().minimize(train_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--puhsxOvjHG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The only thing left is actually training our model.\n",
        "Since we can reuse the hyperparameters from the simple training method. This ensures comparability."
      ]
    },
    {
      "metadata": {
        "id": "w8k0XiRAvnXM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e5c3cb0c-b15c-47d4-e952-6944d4b3ae05",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525533059928,
          "user_tz": -120,
          "elapsed": 191897,
          "user": {
            "displayName": "Michael PenguinMenace",
            "photoUrl": "//lh3.googleusercontent.com/-wv0FkQOEnjg/AAAAAAAAAAI/AAAAAAAAAHM/byTo6iipigo/s50-c-k-no/photo.jpg",
            "userId": "112772895502545919169"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from starttf.utils.plot_losses import DefaultLossCallback\n",
        "from starttf.utils.session_config import get_default_config\n",
        "from starttf.estimators.scientific_estimator import train_and_evaluate\n",
        "\n",
        "with tf.Session(config=get_default_config()) as session:\n",
        "        checkpoint_path = train_and_evaluate(hyper_params, session, train_op,\n",
        "                                             metrics=[train_metrics, validation_metrics],\n",
        "                                             callback=DefaultLossCallback().callback,\n",
        "                                             enable_timing=True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model: To reduce overhead no outputs are done. Use tensorboard to see your progress.\n",
            "python -m tensorboard.main --logdir=checkpoints\n",
            "Timing: 177.183 ms per iteration\n",
            "Iter: 250, Train loss: 0.0493, Test loss: 0.0889\n",
            "Timing: 94.161 ms per iteration\n",
            "Iter: 500, Train loss: 0.0779, Test loss: 0.1184\n",
            "Timing: 94.884 ms per iteration\n",
            "Iter: 750, Train loss: 0.0751, Test loss: 0.1157\n",
            "Timing: 96.057 ms per iteration\n",
            "Iter: 1000, Train loss: 0.0905, Test loss: 0.0910\n",
            "Timing: 96.221 ms per iteration\n",
            "Iter: 1250, Train loss: 0.0078, Test loss: 0.0361\n",
            "Timing: 96.063 ms per iteration\n",
            "Iter: 1500, Train loss: 0.0406, Test loss: 0.1907\n",
            "Timing: 95.023 ms per iteration\n",
            "Iter: 1750, Train loss: 0.1314, Test loss: 0.0044\n",
            "Timing: 95.445 ms per iteration\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd99dbc4940>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "_x5yya7f2HXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations!\n",
        "\n",
        "When you now want to work on a different dataset you have to do basically the same."
      ]
    },
    {
      "metadata": {
        "id": "WEvh33oA2Mxh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How to do that on something else than mnist?\n",
        "\n",
        "1. Write a data generator, like the mnist generator we used. (See [mnist here](https://github.com/penguinmenac3/opendatalake/blob/master/opendatalake/classification/mnist.py) or a named folder based loader [here](https://github.com/penguinmenac3/opendatalake/blob/master/opendatalake/classification/named_folders.py))\n",
        "```python\n",
        "def threadable_gen(params, stride=1, offset=0, infinite=False):\n",
        "        # This function cannot be a lambda or pooling does not work.\n",
        "        # All interactions with the outside world must be via the params object. (agian for pooling to work)\n",
        "        # yield dicts for features and labels\n",
        "```\n",
        "2. Prepare your data as we have done here using the write_data method. You can optionally pass in a data augmentation, label_preprocessing and feature_preprocessing method if you want to.\n",
        "```python\n",
        "def write_data(hyper_params,\n",
        "               prefix,\n",
        "               threadable_generator,\n",
        "               params,\n",
        "               num_threads,\n",
        "               preprocess_feature=None,\n",
        "               preprocess_label=None,\n",
        "               augment_data=None):\n",
        "```\n",
        "3. Write a model or use a predefined one. There are implementations of common models in starttf.models. You can find vgg16 there for example. If you have an awesome model, consider a pull request at [starttf project on github](https://github.com/penguinmenac3/starttf/)\n",
        "```python\n",
        "def create_model(input_tensor, mode, hyper_params):\n",
        "        # Do not forget reuse variables and variable scope!\n",
        "```\n",
        "4. Write a loss like we did here to glue together your model and the labels.\n",
        "```python\n",
        "def create_loss(model, labels, mode, hyper_params):\n",
        "        # Return a loss op and metrics dict (like shown in this notebook)\n",
        "        # If you need some advanced losses, consider using starttf.losses. (alpha balancing, focus loss, mask loss, ...)\n",
        "```\n",
        "5. Use the easy_train_and_evaluate method or write your own training logic. Here we used a train_and_evaluate method, if you need some specific training that it cannot do you can use the source code [here](https://github.com/penguinmenac3/starttf/blob/master/starttf/estimators/scientific_estimator.py) and modify it to your requirements. If it is of general interest and has good code quality, consider pull requesting. ;)\n",
        "```python\n",
        "def easy_train_and_evaluate(hyper_params, create_model, create_loss, init_model=None):\n",
        "        # Init Model is a callback that you can use to initialize your model with pretrained weights just before training starts.\n",
        "```\n",
        "\n",
        "Stick to those patterns and writing a model for example 3d-detection of vehicles in realtime is done with just as many lines of code as writing an mnist network.\n",
        "Trust me on that one. I tried it myself. ;)"
      ]
    }
  ]
}